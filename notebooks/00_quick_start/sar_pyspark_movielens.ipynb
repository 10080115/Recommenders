{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SAR on MovieLens (pySpark)\n",
    "\n",
    "SAR is a fast scalable adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It produces easily explainable / interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. \n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate SAR's pySpark implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run SAR efficiently on Data Science Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb 10 2017, 07:08:35) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)]\n",
      "Spark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from reco_utils.recommender.sar.sar_pyspark import SARpySparkReference\n",
    "from reco_utils.dataset.url_utils import maybe_download\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "from reco_utils.common.notebook_utils import is_jupyter, is_databricks\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "# do not recommend items which appear in the training set\n",
    "RECOMMEND_SEEN = False\n",
    "# dataset size\n",
    "MOVIELENS_DATA_SIZE = \"100k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SAR pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3g\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\")\\\n",
    "    .config(\"spark.memory.stageFraction\", \"0.3\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"36000s\")\\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieLens data have different data-format for each size of dataset\n",
    "data_header = False\n",
    "if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "    separator = \"\\t\"\n",
    "    data_path = \"ml-100k/u.data\"\n",
    "elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-1m/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-10M100K/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "    separator = \",\"\n",
    "    data_path = \"ml-20m/ratings.csv\"\n",
    "    data_header = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid data size. Should be one of {100k, 1m, 10m, or 20m}\")\n",
    "\n",
    "# Download dataset zip file and decompress if haven't done yet\n",
    "dest_folder = \".\"\n",
    "dest_file = data_path\n",
    "\n",
    "if is_databricks():\n",
    "    # Handle local file I/O APIs on Databricks\n",
    "    dest_folder = \"/dbfs/tmp\"\n",
    "    dest_file = os.path.join(dest_folder, dest_file)\n",
    "    data_path = \"dbfs:/tmp/\" + data_path\n",
    "    \n",
    "if not os.path.exists(dest_file):\n",
    "    filename = \"ml-\" + MOVIELENS_DATA_SIZE + \".zip\"\n",
    "    filepath = maybe_download(\n",
    "        \"http://files.grouplens.org/datasets/movielens/\" + filename, filename\n",
    "    )\n",
    "\n",
    "    with ZipFile(filepath, \"r\") as zf:\n",
    "        zf.extractall(dest_folder)\n",
    "\n",
    "    # remove zip file we already used\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Force the file to be flushed to persistent storage\n",
    "if is_databricks():\n",
    "    # In Databricks, passing python variable to shell command like \"!sync {dest_file}\" does not work.\n",
    "    if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "        !sync \"/dbfs/tmp/ml-100k/u.data\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "        !sync \"/dbfs/tmp/ml-1m/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "        !sync \"/dbfs/tmp/ml-10M100K/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "        !sync \"/dbfs/tmp/ml-20m/ratings.csv\"\n",
    "else:\n",
    "    !sync {dest_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(\"UserId\", IntegerType()),\n",
    "        StructField(\"MovieId\", IntegerType()),\n",
    "        StructField(\"Rating\", FloatType()),\n",
    "        StructField(\"Timestamp\", LongType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "# pySpark's read csv currently doesn't support multi-character delimiter, thus we manually handle that\n",
    "if len(separator) > 1:\n",
    "    raw_data = spark.sparkContext.textFile(data_path)\n",
    "    # In databricks (or maybe in multi-cluster machines), somehow file \n",
    "    raw_data.take(1)\n",
    "    data_rdd = raw_data.map(lambda l: l.split(separator)) \\\n",
    "        .map(lambda c: [int(c[0]), int(c[1]), float(c[2]), int(c[3])])\n",
    "    data = spark.createDataFrame(data_rdd, schema)\n",
    "else:\n",
    "    data = spark.read.csv(data_path, schema=schema, sep=separator, header=data_header)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark random splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.count())\n",
    "print (\"N test\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "        \"col_user\": \"UserId\",\n",
    "        \"col_item\": \"MovieId\",\n",
    "        \"col_rating\": \"Rating\",\n",
    "        \"col_timestamp\": \"Timestamp\",\n",
    "    }\n",
    "\n",
    "model = SARpySparkReference(spark=spark,\n",
    "                remove_seen=True, similarity_type=\"jaccard\", \n",
    "                time_decay_coefficient=30, time_now=None, timedecay_formula=True, **header\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cold User filter\n",
    "In order to use SAR, we need to hash users and items and make sure there are no cold users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "train_set_users = set([x[0] for x in train.select(header[\"col_user\"]).distinct().collect()])\n",
    "test_set_users = set([x[0] for x in test.select(header[\"col_user\"]).distinct().collect()])\n",
    "both_sets = train_set_users.intersection(test_set_users)\n",
    "test = test.filter(F.col(header[\"col_user\"]).isin(both_sets))\n",
    "print (\"N train\", train.count())\n",
    "print (\"N test\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build uniform index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query -- select UserId, dense_rank() over(partition by 1 order by UserId) as row_id, MovieId, dense_rank() over(partition by 1 order by MovieId) as col_id, Rating, Timestamp, type from df_all\n"
     ]
    }
   ],
   "source": [
    "# we need to index item IDs which we want to score later, i.e. we need to consider all items\n",
    "train = train.withColumn('type', F.lit(1))\n",
    "test = test.withColumn('type', F.lit(0))\n",
    "df_all = train.union(test)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "\n",
    "# create new index for the items\n",
    "query = \"select \" + header[\"col_user\"] + \", \" +\\\n",
    "    \"dense_rank() over(partition by 1 order by \" + header[\"col_user\"] + \") as row_id, \" +\\\n",
    "                    header[\"col_item\"] + \", \" +\\\n",
    "    \"dense_rank() over(partition by 1 order by \" + header[\"col_item\"] + \") as col_id, \" +\\\n",
    "        header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \", type from df_all\"\n",
    "print(\"Running query -- \" + query)\n",
    "df_all = spark.sql(query)\n",
    "df_all.createOrReplaceTempView(\"df_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recover the original data but now with index built-in\n",
    "Obtain the indexed dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query -- select row_id, col_id, Rating, Timestamp from df_all where type=1\n",
      "Running query -- select row_id, col_id, Rating, Timestamp from df_all where type=0\n"
     ]
    }
   ],
   "source": [
    "query = \"select row_id, col_id, \" + header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \" from df_all where type=1\"\n",
    "print(\"Running query -- \" + query)\n",
    "train_indexed = spark.sql(query)\n",
    "\n",
    "query = \"select row_id, col_id, \" + header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \" from df_all where type=0\"\n",
    "print(\"Running query -- \" + query)\n",
    "test_indexed = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build index mappings: IDs to index and index to IDs.\n",
    "First we obtain all users and items which are used later in SAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users =\\\n",
    "    np.array([x[header[\"col_user\"]] for x in df_all.select(header[\"col_user\"]).distinct().toLocalIterator()])\n",
    "unique_items =\\\n",
    "    np.array([x[header[\"col_item\"]] for x in df_all.select(header[\"col_item\"]).distinct().toLocalIterator()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing users and items: index all rows and columns, then split again into train and test. We perform the reduction on Spark across keys before calling .collect so this is scalable. The assumption is that we can store at least the full list of unique users and unique items on a single machine (vertical scaling).\n",
    "\n",
    "We also reverse the dictionaries in order to go ther othe way. Index to item is used to return top_k DataFrame later by undoing the index. For performance reasons we can use vector array to store index to ID mapping, but we're using dictionary for convenience (both are O(1) access anyway).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2user = \\\n",
    "    dict(df_all.select([\"row_id\", header[\"col_user\"]]).rdd.reduceByKey(lambda _, v: v).collect())\n",
    "index2item = \\\n",
    "    dict(df_all.select([\"col_id\", header[\"col_item\"]]).rdd.reduceByKey(lambda _, v: v).collect())\n",
    "\n",
    "user_map_dict = {v: k for k, v in index2user.items()}\n",
    "item_map_dict = {v: k for k, v in index2item.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the index values in the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_index(unique_users, unique_items, user_map_dict, item_map_dict, index2user, index2item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the SAR model on our training data, and get the top-k recommendations for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_indexed)\n",
    "top_k = model.recommend_k_items(test_indexed, top_k = TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|UserId|MovieId|prediction|\n",
      "+------+-------+----------+\n",
      "|   551|    195| 155.13187|\n",
      "|   551|    161| 148.76941|\n",
      "|   551|    174|  145.6567|\n",
      "|   551|    202| 144.97148|\n",
      "|   551|    173| 142.98204|\n",
      "|   796|     82| 141.05814|\n",
      "|   796|    174| 140.94711|\n",
      "|   796|    202| 140.84773|\n",
      "|   551|    168| 140.36824|\n",
      "|   551|    144| 140.09683|\n",
      "|   551|    216| 139.22217|\n",
      "|   551|    367| 138.58034|\n",
      "|   551|     98| 138.57614|\n",
      "|   796|     96| 137.61952|\n",
      "|   796|    393| 136.54305|\n",
      "|   796|    176| 136.16814|\n",
      "|   796|     28| 134.18773|\n",
      "|   796|    186| 133.72394|\n",
      "|   796|     58| 132.76549|\n",
      "|   796|    402| 132.04024|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinity calculation time (s)\t6\n",
      "item_cooccurrence calculation time (s)\t11\n",
      "item_similarity calculation time (s)\t21\n",
      "scores calculation time (s)\t20\n",
      "masked_scores calculation time (s)\t19\n",
      "top_scores calculation time (s)\t27\n",
      "top_scores calculation time (s)\t32\n"
     ]
    }
   ],
   "source": [
    "# print timer info if in debug mode\n",
    "if model.debug:\n",
    "    for x in model.timer_log:\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate how well SAR performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----+\n",
      "|UserId|MovieId|Rating|Timestamp|type|\n",
      "+------+-------+------+---------+----+\n",
      "|     1|      2|   3.0|876893171|   0|\n",
      "|     1|      3|   4.0|878542960|   0|\n",
      "|     1|      4|   3.0|876893119|   0|\n",
      "|     1|      9|   5.0|878543541|   0|\n",
      "|     1|     11|   2.0|875072262|   0|\n",
      "|     1|     17|   3.0|875073198|   0|\n",
      "|     1|     25|   4.0|875071805|   0|\n",
      "|     1|     28|   4.0|875072173|   0|\n",
      "|     1|     30|   3.0|878542515|   0|\n",
      "|     1|     33|   4.0|878542699|   0|\n",
      "|     1|     43|   4.0|878542869|   0|\n",
      "|     1|     48|   5.0|875072520|   0|\n",
      "|     1|     49|   3.0|878542478|   0|\n",
      "|     1|     52|   4.0|875072205|   0|\n",
      "|     1|     59|   5.0|876892817|   0|\n",
      "|     1|     62|   3.0|878542282|   0|\n",
      "|     1|     65|   4.0|875072125|   0|\n",
      "|     1|     66|   4.0|878543030|   0|\n",
      "|     1|     71|   3.0|876892425|   0|\n",
      "|     1|     78|   1.0|878543176|   0|\n",
      "+------+-------+------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_k, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tsar_pyspark\n",
      "Top K:\t10\n",
      "MAP:\t0.110195\n",
      "NDCG:\t0.378840\n",
      "Precision@K:\t0.327176\n",
      "Recall@K:\t0.178576\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\t\" + model.model_str,\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
