{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RBM Deep Dive with Tensorflow \n",
        "\n",
        "In this notebook we provide a complete walkthough of the Restricted Boltzmann Machine (RBM) algorithm with applications to recommender systems. In particular, we use as a case study the [movielens dataset](https://movielens.org), comprising the ranking of movies (from 1 to 5) given by viewers. A quickstart version of this notebook can be found [here](notebooks/00_quick_start/rbm_movielens.ipynb).  \n",
        "\n",
        "### Overview \n",
        "\n",
        "The RBM is a generative neural network model used to perform unsupervised learning. The main task of an RBM is to learn the joint probability distribution $P(v,h)$, where $v$ are the visible units and $h$ the hidden ones. The hidden units represent latent variables while the visible units are clamped on the input data. Once the joint distribution is learnt, new examples are generated by sampling from it.  \n",
        "\n",
        "The implementation presented here is based on the article by Ruslan Salakhutdinov, Andriy Mnih and Geoffrey Hinton [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) with the exception that here we use multinomial units instead of one-hot encoded. \n",
        "\n",
        "### Advantages of RBM: \n",
        "\n",
        "The model generates ratings for a user/movie pair using a collaborative filtering based approach. While matrix factorization methods learn how to reproduce an instance of the user/item affinity matrix, the RBM learns its underlying probability distribution. This has several advantages: \n",
        "\n",
        "- Generalizability : the model generalize well to new examples as long as they do not differ much in probability\n",
        "- Stability in time: if the recommendation task is time-stationary, the model does not need to be trained often to accomodate new ratings/users. \n",
        "- Scales well with the size of the dataset and the sparsity of the user/affinity matrix. \n",
        "- The tensorflow implementation presented here allows fast, scalable  training on GPU \n",
        "\n",
        "### Outline \n",
        "\n",
        "This notebook is organizaed as follows:\n",
        "\n",
        "1. RBM Theory \n",
        "2. Tensorflow implementation of the RBM \n",
        "3. Data preparation and inspection\n",
        "4. Model application, performance and analysis of the results  \n",
        "\n",
        "Sections 1 and 2 requires basic knowledge of linear algebra, probability theory and tensorflow while  \n",
        "sections 3 and 4 only requires some basic data science understanding. **Feel free to jumpo to the section you are most interested in!**\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 Global Settings and Import"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "# set the environment path to find Recommenders\n",
        "import sys\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import papermill as pm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from reco_utils.recommender.rbm.Mrbm_tensorflow import RBM\n",
        "from reco_utils.dataset.rbm_splitters import splitter\n",
        "\n",
        "from reco_utils.dataset.url_utils import maybe_download\n",
        "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
        "\n",
        "#For interactive mode only\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. RBM Theory \n",
        "\n",
        "## 1.1 Overview and main differences with other recommender algorithms\n",
        "\n",
        "A Restricted Boltzmann Machine (RBM) is an undirected graphical model with origin in the statistical mechanics (or physics) of magnetic systems. Statistical mechanics (SM) provides a probabilistic description of complex systems made of a huge number of constituents (typically $\\sim 10^{23}$); instead of looking at a particular instance of the system, the aim of SM is to describe their **typical** behaviour. This approach has been succesfull for the description of gases, liquids, complex materials (e.g. semiconductors) and even the famous [Higgs boson](https://en.wikipedia.org/wiki/Higgs_boson)!\n",
        "\n",
        "Being designed to handle and organize a large amount of data, SM finds ideal applications in modern learnign algorithms. In the context of **recommender systems**, the idea is to learn typical user behaviour instead of particular instances. To understand this better, consider the most general setup of a recmmendation problem: there are $m$ users rating $n$ items according to some scale (e.g. 1 to 5). In a typical scenario of online shopping, streaming services or decision processes, the user only rates a subset $l \\ll m$ of the products. If we now create a matrix representation of this problem, we obtain the user/item affinity matrix $X$. In a more readable table form, $X$ will look like this:\n",
        "\n",
        "|     |$i_1$  |$i_2$  |$i_3$  |  ... |$i_m$  | \n",
        "|-----|-------|-------|-------|------|-------|\n",
        "|$u_1$|5      |0      |2      |0 ... |1      |\n",
        "|$u_2$|0      |0      |3      |4 ... |0      |\n",
        "|...  |...    |...    |...    |...   |...    |\n",
        "|$u_m$|3      |3      |0      |5...  |2      |\n",
        "\n",
        "where zeroes denote unrated items. In a nutshell, the recommender task is to \"fill in\" the missing ratings (later we will see that in practice this is not the only criteria to recommend a product). The classical approach to this problem is called matrix factorization: the basic idea is to decompose $X$ into a user ($P$) and item ($Q$) matrix, such that $X = Q^T P$. The dimensions of the two matrices are $dim(Q) = (f, n)$ and $dim(P)= (f,m)$ where $f \\le m,n$ is the number of latent factors, e.g. the genre of a movie, the type of food etc... and it is an hyperparameter of the model, for more details see the [ALS notebook](notebooks/02_modeling/als_deep_dive.ipynb). By learning $Q$ and $P$ we try to reproduce a particular instance of $X$ (provided by the available data) and use this information to fill up the missing matrix elements. \n",
        "\nThe RBM approach is to look at $X$ as a particular realization (sample) of a more general process; instead of learning a specific $X$, we try to learn the matrix distrbution from which $X$ has been sampled form. Efectively, we learn the typical distirbution of *tastes* (i.e. latent factors) and use this information to *generate* new ratings. For this reaoson, this class of neural network models is also called **generative**. Consider the following example: imagine you are given the  income distribution per age window of a particular country (this is easy to find from goverments data), then we could fix the age window and *generate* virtual citizens with various incomes by sampling from this distibution.   "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model \n",
        "\n",
        "The central quantity of every SM model is the [Boltzmann distribution ](https://en.wikipedia.org/wiki/Boltzmann_distribution); this can be seen as the least biased probability distribution on a certain probability space $\\Sigma$ and can be obtained using a maximum entropy principle on the space of distributions over $\\Sigma$. Its typical form is: \n",
        "\n",
        "$P = \\frac{1}{Z} \\, e^{- \\beta \\, H}$, \n",
        "\n",
        "where $Z$ is a normalization constant known as the partition function, $\\beta$ is a noise parameter with units of inverse energy and $H$ is the Hamiltonian, or energy function of the system. For this reason, this class of models is also known as *energy based* in computer science. In physics, $\\beta$ is the inverse temperature of the system in units of Boltzmann's constant, but here we will effectively rescale inside $H$, so that this is now a pure number. $H$ describes the behaviour of two sets of stochastic vectors, typically called $v_i$ (visibles) and $h_j$ (hidden). The former constitutes both the input *and* the ouput of the algo (this will be clear later), while the hidden units are the latent factors we want to learn. This structure results in the following Neural Network topology\n",
        "\n\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Data preparation and inspection \n",
        "\n",
        "The Movielens dataset comes in different sizes, denoting the number of available ratings. The number of users and rated movies also changes across the different dataset. The data are imported in a pandas dataframe including the **user ID**, the **item ID**, the **ratings** and a **timestamp** denoting when a particular user rated a particular item. Although this last feature could be explicitely included, it will not be considered here. The underlying assumption of this choice is that user's tastes are weakly time dependent, i.e. a user's taste typically chage on time scales (usually years) much longer than the typical recommendation time scale (e.g. hours/days). As a consequence, the joint probability distribution we want to learn can be safely considered as time dependent. Nevertheless, timestamps could be used as *contextual variables*, e.g. recommend a certain movie during the weekend and another during weekdays.  \n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}