{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RBM Deep Dive with Tensorflow \n",
        "\n",
        "In this notebook we provide a complete walkthough of the Restricted Boltzmann Machine (RBM) algorithm with applications to recommender systems. In particular, we use as a case study the [movielens dataset](https://movielens.org), comprising the ranking of movies (from 1 to 5) given by viewers. A quickstart version of this notebook can be found [here](notebooks/00_quick_start/rbm_movielens.ipynb).  \n",
        "\n",
        "### Overview \n",
        "\n",
        "The RBM is a generative neural network model used to perform unsupervised learning. The main task of an RBM is to learn the joint probability distribution $P(v,h)$, where $v$ are the visible units and $h$ the hidden ones. The hidden units represent latent variables while the visible units are clamped on the input data. Once the joint distribution is learnt, new examples are generated by sampling from it.  \n",
        "\n",
        "The implementation presented here is based on the article by Ruslan Salakhutdinov, Andriy Mnih and Geoffrey Hinton [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) with the exception that here we use multinomial units instead of one-hot encoded. \n",
        "\n",
        "### Advantages of RBM: \n",
        "\n",
        "The model generates ratings for a user/movie pair using a collaborative filtering based approach. While matrix factorization methods learn how to reproduce an instance of the user/item affinity matrix, the RBM learns its underlying probability distribution. This has several advantages: \n",
        "\n",
        "- Generalizability : the model generalize well to new examples as long as they do not differ much in probability\n",
        "- Stability in time: if the recommendation task is time-stationary, the model does not need to be trained often to accomodate new ratings/users. \n",
        "- Scales well with the size of the dataset and the sparsity of the user/affinity matrix. \n",
        "- The tensorflow implementation presented here allows fast, scalable  training on GPU \n",
        "\n",
        "### Outline \n",
        "\n",
        "This notebook is organizaed as follows:\n",
        "\n",
        "1. RBM Theory \n",
        "2. Tensorflow implementation and model parameters  \n",
        "3. Data preparation and inspection\n",
        "4. Model application, performance and analysis of the results  \n",
        "\n",
        "Sections 1 and 2 requires basic knowledge of linear algebra, probability theory and tensorflow while  \n",
        "sections 3 and 4 only requires some basic data science understanding. **Feel free to jumpo to the section you are most interested in!**\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 Global Settings and Import"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "# set the environment path to find Recommenders\n",
        "import sys\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import papermill as pm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from reco_utils.recommender.rbm.Mrbm_tensorflow import RBM\n",
        "from reco_utils.dataset.rbm_splitters import splitter\n",
        "\n",
        "from reco_utils.dataset.url_utils import maybe_download\n",
        "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
        "\n",
        "#For interactive mode only\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. RBM Theory \n",
        "\n",
        "## 1.1 Overview and main differences with other recommender algorithms\n",
        "\n",
        "A Restricted Boltzmann Machine (RBM) is an undirected graphical model with origin in the statistical mechanics (or physics) of magnetic systems. Statistical mechanics (SM) provides a probabilistic description of complex systems made of a huge number of constituents (typically $\\sim 10^{23}$); instead of looking at a particular instance of the system, the aim of SM is to describe their **typical** behaviour. This approach has been succesfull for the description of gases, liquids, complex materials (e.g. semiconductors) and even the famous [Higgs boson](https://en.wikipedia.org/wiki/Higgs_boson)!\n",
        "\n",
        "Being designed to handle and organize a large amount of data, SM finds ideal applications in modern learnign algorithms. In the context of **recommender systems**, the idea is to learn typical user behaviour instead of particular instances. To understand this better, consider the most general setup of a recmmendation problem: there are $m$ users rating $n$ items according to some scale (e.g. 1 to 5). In a typical scenario of online shopping, streaming services or decision processes, the user only rates a subset $l \\ll m$ of the products. If we now create a matrix representation of this problem, we obtain the user/item affinity matrix $X$. In a more readable table form, $X$ will look like this:\n",
        "\n",
        "|     |$i_1$  |$i_2$  |$i_3$  |  ... |$i_m$  | \n",
        "|-----|-------|-------|-------|------|-------|\n",
        "|$u_1$|5      |0      |2      |0 ... |1      |\n",
        "|$u_2$|0      |0      |3      |4 ... |0      |\n",
        "|...  |...    |...    |...    |...   |...    |\n",
        "|$u_m$|3      |3      |0      |5...  |2      |\n",
        "\n",
        "where zeroes denote unrated items. In a nutshell, the recommender task is to \"fill in\" the missing ratings (later we will see that in practice this is not the only criteria to recommend a product). The classical approach to this problem is called matrix factorization: the basic idea is to decompose $X$ into a user ($P$) and item ($Q$) matrix, such that $X = Q^T P$. The dimensions of the two matrices are $dim(Q) = (f, n)$ and $dim(P)= (f,m)$ where $f \\le m,n$ is the number of latent factors, e.g. the genre of a movie, the type of food etc... and it is an hyperparameter of the model, for more details see the [ALS notebook](notebooks/02_modeling/als_deep_dive.ipynb). By learning $Q$ and $P$ we try to reproduce a particular instance of $X$ (provided by the available data) and use this information to fill up the missing matrix elements. \n",
        "\nThe RBM approach is to look at $X$ as a particular realization (sample) of a more general process; instead of learning a specific $X$, we try to learn the matrix distrbution from which $X$ has been sampled form. Efectively, we learn the typical distirbution of *tastes* (i.e. latent factors) and use this information to *generate* new ratings. For this reaoson, this class of neural network models is also called **generative**. Consider the following example: imagine you are given the  income distribution per age window of a particular country (this is easy to find from goverments data), then we could fix the age window and *generate* virtual citizens with various incomes by sampling from this distibution.   "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model \n",
        "\n",
        "The central quantity of every SM model is the [Boltzmann distribution ](https://en.wikipedia.org/wiki/Boltzmann_distribution); this can be seen as the least biased probability distribution on a given probability space $\\Sigma$ and can be obtained using a maximum entropy principle on the space of distributions over $\\Sigma$. Its typical form is: \n",
        "\n",
        "$P = \\frac{1}{Z} \\, e^{- \\beta \\, H}$, \n",
        "\n",
        "where $Z$ is a normalization constant known as the partition function, $\\beta$ is a noise parameter with units of inverse energy and $H$ is the Hamiltonian, or energy function of the system. For this reason, this class of models is also known as *energy based* in computer science. In physics, $\\beta$ is the inverse temperature of the system in units of Boltzmann's constant, but here we will effectively rescale inside $H$, so that this is now a pure number. $H$ describes the behaviour of two sets of stochastic vectors, typically called $v_i$ (visibles) and $h_j$ (hidden). The former constitutes both the input *and* the ouput of the algo (this will be clear later), while the hidden units are the latent factors we want to learn. This structure results in the following Neural Network topology\n",
        "\n",
        "![rbm1](https://393229b54263dsvm.file.core.windows.net/images/RBM1.png?sp=rl&st=2018-12-03T13:06:28Z&se=2020-12-04T13:06:00Z&sv=2017-11-09&sig=XP5y%2BXuU4r3Snmb%2FIgksMR%2B26XZfgrV2ZVs183Nwn3k%3D&sr=f)\n",
        "\n",
        "The input of the movielens databse consists of ratings from 1 to 5; we shall thus consider a discrete configuration space of $m$ visible variables, each taking values in a finite set $\\chi_v = \\{ 1, 2, 3,4,5 \\}$. A global configuration of the system is determined by $\\mathbf{v} = (v_1, v_2, ..., v_m) \\in \\chi_v^m$ and we reserve $0$ for an unrated movie. We also need to specify the hidden units, that we take as random binary variables $\\chi_h = \\{0,1 \\}$ denoting if the particular unit is active or not and $\\mathbf{h} = (h_1, h_2, ...,h_n) \\in \\chi_h^n$. As we said before, the hidden units may describe attributes such as the genre of a movie; so given for example an sci-fi/horror movie, only the hidden units describing such attribute should be active. The minimal model for such a system is defined by the following Hamiltonian: \n",
        "\n",
        "$H = - \\sum_{i,j \\in G} v_i \\, w_{ij} \\, h_j - \\sum_{i=1}^m v_i \\, a_i - \\sum_{j=1}^n h_i \\, b_i$\n",
        "\n",
        "The first term is an \"interaction term\", capturing the correlations between the visible and hidden units, while the other two terms are \"potential terms\", taking into account the bias of the units. The correlation matrix $w_{ij}$ and the two biases $a_i$ and $b_i$ are learning parameters to be fixed by the minimization of a properly defined cost function. Remember that this is an unsupervised problem, i.e. there is no real output and therefore we cannot directly minimize the error function between the prediction and the labeled data. As in every SM problem, the right quantity to minimize is the Free energy (remember that $\\beta =1$ and there is a minus sign difference for convention)\n",
        "\n",
        "$F = \\log Z = \\log \\sum_{ v_i, h_i } P(v, h)$.\n",
        "\nIn the langiage of probaility theory, the above quantity is the cumulant generating function. One way of evaluating the free energy is to use a [Markov-chain Montecarlo sampling](https://en.wikipedia.org/wiki/Monte_Carlo_method#Computer_graphics) algorithm such as the Metropolis-Hasting; here we will use instead an approximate method called Contrastive divergence, based on [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) (see below). The latter algorithm has the advantage of being faster than the Montecarlo ones. Once the candidate $F$ has been found, we fix the learning parameters by extremizing $F$. Let us see how this works in practive in the next section. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Learning Algorithm \n",
        "\n",
        "Instead of sampling directly from the joint probability distirbution, one can evaluate the conditional distributions instead   \n",
        "\n",
        "$P(v, h) = P(v|h) P(h) = P(h|v) P(v)$ \n",
        "\n",
        "where the second equality follows from the fact that the model is undirected or, in physical terms, is at equilibrium. Gibbs sampling eesentially consists of two steps called positive and negative phases:\n",
        "\n",
        "### Positive \n",
        "\n",
        "**Fix the visible units on the data and evaluate $P(h_j =1| \\mathbf{v})$**, i.e. the probability that the jth hidden unit is active given the entire input vector. In practice, it is convenient to evaluate the generating function: \n",
        "\n",
        "$ Z[v,b] = \\prod_j \\sum_{h_j = 0,1}  e^{(\\sum_i w_{ij} v_i + b_j) h_j} = \\prod_j \\left( 1+  e^{\\sum_i w_{ij} v_i + b_j} \\right)$.\n",
        "\n",
        "Taking the gradients with respect to the bias we easily obtain \n",
        "\n",
        "$\\frac{\\partial}{\\partial b_j}\\log Z[v,b] =  \\frac{1}{1+ e^{-(\\sum_i w_{ij} v_i + b_j)}} = \\sigma( \\phi_j(v, b) )$\n",
        "\n",
        "where $\\phi_j(v,b) = \\sum_i w_{ij} v_i + b_j $ and we have identified the logistic function $\\sigma(.) \\equiv P(h_j=1|v,b)$. \n",
        "\n",
        "**Use $\\sigma$ to sample the value of $h_j$** \n",
        "\n",
        "### Negative \n",
        "\n",
        "**Use the sampled value of the hidden units to evaluete P(v_i = q |h)**, where $q=1,...,5$. This is given by the multinomial expression\n",
        "\n",
        "$P(v_i = q |h,a) = e^{v_i (\\sum_j w_{ij} \\, h_j + a_i ) }/ \\prod_{v_i=1}^q e^{v_i (\\sum_j w_{ij} \\, h_j + a_i ) } $\n",
        "\n",
        "Finally, sample the values of $v_i$ from the above distribution. Clearly, these new $v_i$ are not necessarily those we have used as an input, at least not at the beginning of training. The above steps are repeated $k$ times, where $k$ is usually increased during training according to a specified protocol. \n",
        "\n",
        "At the end of each k-step Gibbs sampling, we evaluate the difference between the initial free energy ($k=0$) (given v) and the one after k-steps \n",
        "\n",
        "$\\Delta F = F_0 - F_k$, \n",
        "\n",
        "and update the learning parameters $w_{ij}$, $b_i$ and $a_i$: \n",
        "\n",
        "$\\frac{\\partial}{\\partial b_j} \\Delta F = \\frac{\\partial}{\\partial b_j} (\\log Z_0[v,b] - \\log Z_k[v,b]) = P_0(h_j=1|v,b) - P_k(h_j=1|v,b)$\n",
        "\n",
        "$\\frac{\\partial}{\\partial w_{ij} } \\Delta F = v_i \\, P_0(v_i = q|h, a) - v_i P_k(v_i| h,a) \\equiv \\langle v_i\\rangle_0 - \\langle v_i \\rangle_k$. \n",
        "\n",
        "This process is repeated for each training epoch, eventually until $\\Delta F =0$, i.e. the learned distribution faithfully reproduces the empirical one. In this sense, the $v_i$ serve both as input and output of the model. As $w_{ij}$ contains informations on how users' votes are correlated, we can use this information to generate ratings for the unseen movies by sampling from the learned, marginal distribution:\n",
        "\n",
        "$\\langle v_i \\rangle = \\sum_{v_i} v_i \\, P(v)$ \n",
        "\n",
        "The entire workflow is summarised below \n",
        "\n",
        "![gibbs](https://393229b54263dsvm.file.core.windows.net/images/Gsampling.001.png?sp=rl&st=2018-12-04T08:34:33Z&se=2020-12-05T08:34:00Z&sv=2017-11-09&sig=N0GXdj0UqLsXM2cwIZsWdMgqUTlYvu1pmGhnNmuAOWI%3D&sr=f)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. TensorFlow implemetation and model parameters \n",
        "\n",
        "In this section we briefly describe how the algorithm in implemented in Tensorflow and which paramters can be customized by the user during training. We also discuss some best practice to be used when training the RBM model on a recommendation task. Further technical details are explained directly in the [code](reco_utils/recommender/rbm/Mrbm_tensorflow.py). \n",
        "\n",
        "Tensorflow (TF) is an opensource framework to develope deep learning (DL) models in a fast and efficient way. One of the shared characteristics of DL frameworks is autodifferentiation, i.e. the symbolic evaluation of gradients, that will be particulary useful here. The other advantage of TF is the geration and optimization of the symbolic operations defined on a computational graph, for fast and scalable deployment on CPU and GPU. For more informations on TF see [here](www.tensorflow.com). Unfortunately, TF is tailor made for supervised learning tasks, so its application to unsupervised model needs some more work. Note: although TF has recently started developing a [set of libraries to perform probabilistic inference](www.tensorflow_probability.com), we found their performance still not optimal and therefore we will not use them here. \n",
        "\n\n\n",
        "The model is instantiated as a class with several methods to build the graph, perform sampling, training and inference. The skeleton of the graph is built at the moment the class is instantiated; mandatory fields are \n",
        "\n",
        "- `hidden_units`: number of hidden units\n",
        "- `training_epoch`: number of training epochs \n",
        "- `minibatch_size`: size of the batch to be chosen at random at each training epoch \n",
        "\n",
        "The optional parameters are: \n",
        "\n",
        "- `keep_prob` : float (Default = 0.7) we use dropout regularization on the hidden units, so this parameters specifies the probability of keeping the connection to a hidden unit active. Dropout will affect specific matrix elements of $w_{ij}$, decreasing in this way the model's complexity and generally improving generalization. \n",
        "\n",
        "- `momentum` : float (Default= 0.5 - 0.9) We use TF's native momentum optimizer.  \n",
        "\n",
        "- `CD_protocol` : Array (Default = $[50, 70, 80,90,100]$) perfecentage of the entire training epochs when the \"annealing\" parameter should be increased. We use this protocol to increase the k-sampling step. In the default case, the first 50% of the training epochs are sampled with a single k-step. As training converges, the number of k-steps is increased by $1$ at each percentage.\n",
        "\n",
        "- `save_model`: Boolean (Default - False) if True save the model to a TF .ckpt file to be restored later on during inference. \n",
        "\n",
        "- `save_path`: String (Default = 'reco_utils/recommender/rbm/saver'), spefies the path where the RF model is goinf to be saved\n",
        "\n",
        "- `debug`: Boolean (Default = False) if True print the output of some of the intermediate steps to inspect their output \n",
        "\n",
        "- `with_metrics`: Boolean (Default= True) if True it evaluates, print and finally plot the mean squared root error per training epoch on the training set. At the end, it also evaluates and print the total model accuracy both on the training and test set. We suggest to switch it off only for benchmarking execution time.  \n",
        "\n",
        "- `init_stdv`: float (Default = 0.05) standard deviation used to inititialize the correlation matrix. \n",
        "\n",
        "- `learning_rate`: float (Default = 0.04) init learning rate used in the optimization algorithm. Note that the optimizer uses a different rate scaled to the batch size $\\alpha$ = `learning_rate/minibatch_size`. \n",
        "\n",
        "- `display_epoch `: integer (Default = 10) the number of epochs after which the rmse error is printed during the learning phase. \n",
        "\n",
        "Although optional, it is likely that the \"annealing protocol\" needs to be modified for different recommenders, we suggest the user to keep this in mind when training the recommender on a new dataset. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Data preparation and inspection \n",
        "\n",
        "The Movielens dataset comes in different sizes, denoting the number of available ratings. The number of users and rated movies also changes across the different dataset. The data are imported in a pandas dataframe including the **user ID**, the **item ID**, the **ratings** and a **timestamp** denoting when a particular user rated a particular item. Although this last feature could be explicitely included, it will not be considered here. The underlying assumption of this choice is that user's tastes are weakly time dependent, i.e. a user's taste typically chage on time scales (usually years) much longer than the typical recommendation time scale (e.g. hours/days). As a consequence, the joint probability distribution we want to learn can be safely considered as time dependent. Nevertheless, timestamps could be used as *contextual variables*, e.g. recommend a certain movie during the weekend and another during weekdays.  \n",
        "\n",
        "Below, we first load the different movielens data in pandas dataframes, explain how the user/affinity matrix is built and how the train/test set is generated. As this procedure is common to all the datasets we will consider, we explain it in details only for the 1 milion dataset.  \n",
        "\nWe start with downloading the different dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# MovieLens data have different data-format for each size of dataset \n",
        "data_header = None\n",
        "if MOVIELENS_DATA_SIZE == '100k':\n",
        "    separator = '\\t'\n",
        "    data_name = 'u.data'\n",
        "    data_folder = 'ml-100k'\n",
        "elif MOVIELENS_DATA_SIZE == '1m':\n",
        "    separator = '::'\n",
        "    data_name = 'ratings.dat'\n",
        "    data_folder = 'ml-1m'\n",
        "elif MOVIELENS_DATA_SIZE == '10m':\n",
        "    separator = '::'\n",
        "    data_name = 'ratings.dat'\n",
        "    data_folder = 'ml-10M100K'\n",
        "elif MOVIELENS_DATA_SIZE == '20m':\n",
        "    separator = ','\n",
        "    data_name = 'ratings.csv'\n",
        "    data_folder = 'ml-20m'\n",
        "    data_header = 0\n",
        "else:\n",
        "    raise ValueError('Invalid data size. Should be one of {100k, 1m, 10m, or 20m}') \n",
        "\n",
        "# Download dataset zip file and decompress if haven't done yet\n",
        "data_path = os.path.join(data_folder, data_name)\n",
        "if not os.path.exists(data_path):\n",
        "    filename = 'ml-' + MOVIELENS_DATA_SIZE + '.zip'\n",
        "    filepath = maybe_download('http://files.grouplens.org/datasets/movielens/'+filename, filename)\n",
        "\n",
        "    with ZipFile(filepath, 'r') as zf:\n",
        "        zf.extractall()\n",
        "    \n",
        "    # remove zip file we already used\n",
        "    os.remove(filepath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "MOVIELENS_DATA_SIZE = '100k'\n",
        "\n",
        "mldf_100k = pd.read_csv(\n",
        "    data_path,\n",
        "    sep=separator,\n",
        "    engine='python',\n",
        "    names=['userID','movieID','rating','timestamp'],\n",
        "    header=data_header\n",
        ")\n",
        "mldf_100k.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MOVIELENS_DATA_SIZE = '1m'\n",
        "\n",
        "mldf_1m = pd.read_csv(\n",
        "    data_path,\n",
        "    sep=separator,\n",
        "    engine='python',\n",
        "    names=['userID','movieID','rating','timestamp'],\n",
        "    header=data_header\n",
        ")\n",
        "mldf_1m.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MOVIELENS_DATA_SIZE = '10m'\n",
        "\n",
        "mldf_10m = pd.read_csv(\n",
        "    data_path,\n",
        "    sep=separator,\n",
        "    engine='python',\n",
        "    names=['userID','movieID','rating','timestamp'],\n",
        "    header=data_header\n",
        ")\n",
        "mldf_10m.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MOVIELENS_DATA_SIZE = '20m'\n",
        "\n",
        "mldf_20m = pd.read_csv(\n",
        "    data_path,\n",
        "    sep=separator,\n",
        "    engine='python',\n",
        "    names=['userID','movieID','rating','timestamp'],\n",
        "    header=data_header\n",
        ")\n",
        "mldf_20m.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Split the data using the stratified splitter  \n",
        "\n",
        "As a second step we split the data into train and test set. If you are familiar with training supervised learning model, here you will notice the first difference. In the former case, we cut off a certain proportion of training examples from the dataset (e.g. images), here corresponding to users (or items), ending up with two matrices (train and test) having different row dimensions. To make the RBM works corerctly, we need to mantain the same matrix size for the train and test set, but the two will contain different amounts of ratings. To see this in practice, let us describe what the `rbm_splitters` class does in details: \n",
        "\n",
        "- First, the class creates the $(m,n)$ user/affinity matrix $X$ defined in section **1.1**, together with the sparsness percentage. For example, for the 1m dataset, $95$% of the matrix entries are zeros. This represent a challenge for the learning task: fixing $95% of entries with only $5$% of data points. \n",
        "\n",
        "- Second, the matrix is split into train and test set. By default, we choose a $75$% to $25$% ratio. The `stratified_split()` method selects, for every user, $25$% of rated movies and it moves them in the new test matrix. Following the example $X$ above, we have\n",
        "\n",
        "### Train\n",
        "\n",
        "|     |$i_1$  |$i_2$  |$i_3$  |  ... |$i_n$  |    \n",
        "|-----|-------|-------|-------|------|-------|\n",
        "|$u_1$|0      |0      |2      |0 ... |0      |\n",
        "|$u_2$|0      |0      |3      |0...  |0      |\n",
        "|...  |...    |...    |...    |...   |...    |\n",
        "|$u_m$|3      |0      |0      |0...  |2      |\n",
        "\n\n",
        "### Test \n",
        "\n",
        "|     |$i_1$  |$i_2$  |$i_3$  |  ... |$i_n$  | \n",
        "|-----|-------|-------|-------|------|-------|\n",
        "|$u_1$|5      |0      |0      |0 ... |1      |\n",
        "|$u_2$|0      |0      |0      |4 ... |0      |\n",
        "|...  |...    |...    |...    |...   |...    |\n",
        "|$u_m$|0      |3      |0      |5...  |0      |\n",
        "\n",
        "The Train and Test matrices have exactly the same dimensions (i.e. same numbers of users and movies) but contain different ratings. Once the model is trained, at inference time, we use the test set user vectors to obtain the inferred values for the ratings.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#to use standard names across the analysis \n",
        "header = {\n",
        "        \"col_user\": \"userID\",\n",
        "        \"col_item\": \"movieID\",\n",
        "        \"col_rating\": \"rating\",\n",
        "    }\n",
        "\n",
        "#instantiate the splitter \n",
        "split1m = splitter(DF = mldf_1m, **header)\n",
        "\n",
        "#Returns the train/test matrices (Xtr, Xtst) together with their dataframe versions (train_df, test_df) needed later for validation. \n",
        "#Finally, maps contains the mapping between matrix elements and the original user/movied IDs. \n",
        "\nXtr_1m, Xtst_1m, train_df_1m, test_df_1m, maps_1m = split1m.stratified_split()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful to inspect the distribution of ratings in the test/train matrix to make sure that the splitter keeps the same one across the train and test set. We can inspect this by plotting the normalized histogram"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Xtr_1m[np.where(Xtr_1m !=0)], 5, density= True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Xtst_1m[np.where(Xtst_1, !=0)], 5, density= True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now repeat the same operations for the other datasets"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#100k\n",
        "split100k = splitter(DF = mldf_100k, **header)\n",
        "Xtr_100k, Xtst_100k, train_df_100k, test_df_100k, maps_100k = split1m.stratified_split()\n",
        "\n",
        "#10m\n",
        "split10m = splitter(DF = mldf_10m, **header)\n",
        "Xtr_10m, Xtst_10m, train_df_10m, test_df_10m, maps_10m = split1m.stratified_split()\n",
        "\n",
        "#20m\n",
        "split20m = splitter(DF = mldf_20m, **header)\n",
        "Xtr_20m, Xtst_20m, train_df_20m, test_df_20m, maps_10m = split1m.stratified_split()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Xtr_100k[np.where(Xtr_100k !=0)], 5, density= True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Xtr_10m[np.where(Xtr_10m !=0)], 5, density= True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Xtr_20m[np.where(Xtr_20m !=0)], 5, density= True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model application, performance and analysis of the results  \n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}