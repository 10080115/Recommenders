{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SAR on MovieLens (sarplus)\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate SAR using sarplus, which is based on PySpark (SQL + C++).\n",
    "\n",
    "The biggest advantage of sarplus is scalability of the prediction part, as it's the only limitation is that the similiarity matrix needs to fit in memory on each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "Pandas version: 0.23.4\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from reco_utils.dataset.url_utils import maybe_download\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRankingEvaluation\n",
    "from reco_utils.common.notebook_utils import is_jupyter, is_databricks\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pysarplus import SARPlus\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "\n",
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "# do not recommend items which appear in the training set\n",
    "RECOMMEND_SEEN = True\n",
    "# dataset size\n",
    "MOVIELENS_DATA_SIZE = \"100k\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Start Spark and load sar+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark = (\\n    SparkSession.builder.appName(\"sample\")\\n    .master(\"local[*]\")\\n    .config(\"memory\", \"16G\")\\n    .config(\"spark.sql.shuffle.partitions\", \"10\")\\n    .config(\"spark.sql.crossJoin.enabled\", True)\\n    .config(\"spark.ui.enabled\", False)\\n    .getOrCreate()\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBMIT_ARGS = \"--packages eisber:sarplus:0.2.2 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"SAR pySpark\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.cores\", 8)\n",
    "    .config(\"spark.executor.memory\", \"48g\")\n",
    "    .config(\"spark.executor.instances\", 1)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.crossJoin.enabled\", True)\n",
    "    .config(\"spark.ui.enabled\", False)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"sample\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"memory\", \"16G\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.crossJoin.enabled\", True)\n",
    "    .config(\"spark.ui.enabled\", False)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MovieLens data have different data-format for each size of dataset\n",
    "data_header = False\n",
    "if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "    separator = \"\\t\"\n",
    "    data_path = \"ml-100k/u.data\"\n",
    "elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-1m/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-10M100K/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "    separator = \",\"\n",
    "    data_path = \"ml-20m/ratings.csv\"\n",
    "    data_header = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid data size. Should be one of {100k, 1m, 10m, or 20m}\")\n",
    "\n",
    "# Download dataset zip file and decompress if haven't done yet\n",
    "dest_folder = \".\"\n",
    "dest_file = data_path\n",
    "\n",
    "if is_databricks():\n",
    "    # Handle local file I/O APIs on Databricks\n",
    "    dest_folder = \"/dbfs/tmp\"\n",
    "    dest_file = os.path.join(dest_folder, dest_file)\n",
    "    data_path = \"dbfs:/tmp/\" + data_path\n",
    "    \n",
    "if not os.path.exists(dest_file):\n",
    "    filename = \"ml-\" + MOVIELENS_DATA_SIZE + \".zip\"\n",
    "    filepath = maybe_download(\n",
    "        \"http://files.grouplens.org/datasets/movielens/\" + filename, filename\n",
    "    )\n",
    "\n",
    "    with ZipFile(filepath, \"r\") as zf:\n",
    "        zf.extractall(dest_folder)\n",
    "\n",
    "    # remove zip file we already used\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Force the file to be flushed to persistent storage\n",
    "if is_databricks():\n",
    "    # In Databricks, passing python variable to shell command like \"!sync {dest_file}\" does not work.\n",
    "    if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "        !sync \"/dbfs/tmp/ml-100k/u.data\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "        !sync \"/dbfs/tmp/ml-1m/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "        !sync \"/dbfs/tmp/ml-10M100K/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "        !sync \"/dbfs/tmp/ml-20m/ratings.csv\"\n",
    "else:\n",
    "    !sync {dest_file}\n",
    "    \n",
    "# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(\"UserId\", IntegerType()),\n",
    "        StructField(\"MovieId\", IntegerType()),\n",
    "        StructField(\"Rating\", FloatType()),\n",
    "        StructField(\"Timestamp\", LongType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "# pySpark's read csv currently doesn't support multi-character delimiter, thus we manually handle that\n",
    "if len(separator) > 1:\n",
    "    raw_data = spark.sparkContext.textFile(data_path)\n",
    "    # In databricks (or maybe in multi-cluster machines), somehow file \n",
    "    raw_data.take(1)\n",
    "    data_rdd = raw_data.map(lambda l: l.split(separator)) \\\n",
    "        .map(lambda c: [int(c[0]), int(c[1]), float(c[2]), int(c[3])])\n",
    "    data = spark.createDataFrame(data_rdd, schema)\n",
    "else:\n",
    "    data = spark.read.csv(data_path, schema=schema, sep=separator, header=data_header)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the python random splitter provided in utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = spark_random_split(data)\n",
    "header = {\n",
    "        \"col_user\": \"UserId\",\n",
    "        \"col_item\": \"MovieId\",\n",
    "        \"col_rating\": \"Rating\",\n",
    "        \"col_timestamp\": \"Timestamp\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the SAR model on our training data, and get the top-k recommendations for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sarplus:sarplus.fit 1/2: compute item cooccurences...\n",
      "INFO:sarplus:sarplus.fit 2/2: compute similiarity metric jaccard...\n",
      "INFO:sarplus:sarplus.recommend_k_items 1/3: create item index\n",
      "INFO:sarplus:sarplus.recommend_k_items 2/3: prepare similarity matrix\n",
      "INFO:sarplus:sarplus.recommend_k_items 3/3: compute recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|UserId|MovieId|     score|\n",
      "+------+-------+----------+\n",
      "|   148|     28| 0.5874176|\n",
      "|   148|    238|0.58822286|\n",
      "|   148|    210|  0.593227|\n",
      "|   148|     79| 0.5953816|\n",
      "|   148|    191|0.60170484|\n",
      "|   148|    168|0.61572987|\n",
      "|   148|    172| 0.6162293|\n",
      "|   148|    423|0.62143284|\n",
      "|   148|    195|0.63030726|\n",
      "|   148|    174|0.63451064|\n",
      "|   496|     95| 0.5524776|\n",
      "|   496|     69|0.55337024|\n",
      "|   496|    403| 0.5535351|\n",
      "|   496|    238| 0.5562565|\n",
      "|   496|    173| 0.5616334|\n",
      "|   496|    176|0.56371635|\n",
      "|   496|     79|0.56943566|\n",
      "|   496|    172| 0.5702657|\n",
      "|   496|    210|0.57208157|\n",
      "|   496|    423|0.58741516|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_train = spark.createDataFrame(train)\n",
    "#df_test = spark.createDataFrame(test)\n",
    "\n",
    "model = SARPlus(spark, **header)\n",
    "model.fit(df_train, similarity_type='jaccard', \n",
    "          time_decay_coefficient=30, time_now=None, timedecay_formula=True)\n",
    "\n",
    "top_k = model.recommend_k_items(df_test, 'sarplus_cache', top_k=TOP_K, remove_seen=RECOMMEND_SEEN)\n",
    "\n",
    "top_k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well SAR performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K:\t10\n",
      "MAP:\t0.110195\n",
      "NDCG:\t0.378840\n",
      "Precision@K:\t0.327176\n",
      "Recall@K:\t0.178576\n"
     ]
    }
   ],
   "source": [
    "rank_eval = SparkRankingEvaluation(df_test, top_k, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"score\", \n",
    "                                    relevancy_method=\"top_k\")\n",
    "\n",
    "print(\"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_full)",
   "language": "python",
   "name": "reco_full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
